{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential libraries\n",
    "import pandas as pd  # Pandas for data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # Matplotlib for plotting graphs\n",
    "import seaborn as sns  # Seaborn for statistical data visualization\n",
    "import zipfile  # Zipfile for handling ZIP archive files\n",
    "from autogluon.tabular import TabularPredictor  # AutoGluon for automated machine learning\n",
    "from sklearn.model_selection import train_test_split  # Scikit-learn for splitting datasets\n",
    "import os  # OS module for operating system interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())  # Print the current working directory\n",
    "os.chdir(\"/workspace/\")  # Change the current working directory to '/workspace/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a ZIP file containing dataset\n",
    "!wget \"https://raw.githubusercontent.com/Call-for-Code/Spot-Challenge-Wildfires/main/data/Jan_30-with_historical_weather_forecasts_refreshed_again_on Jan_31.zip\"\n",
    "\n",
    "# Open and extract the contents of the ZIP file\n",
    "zip = zipfile.ZipFile(\"Jan_30-with_historical_weather_forecasts_refreshed_again_on Jan_31.zip\")\n",
    "zip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for datasets\n",
    "main_path = \"Jan_30\"\n",
    "file_wildfires = f\"{main_path}/Historical_Wildfires.csv\"\n",
    "\n",
    "# Load the wildfires dataset into a pandas DataFrame\n",
    "wildfires_df = pd.read_csv(file_wildfires)  \n",
    "wildfires_df[\"Date\"] = pd.to_datetime(wildfires_df[\"Date\"])  # Convert the 'Date' column to datetime format for easier manipulation\n",
    "wildfires_df.head()  # Display the first few rows of the wildfires DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wildfires_df.describe().transpose()  # Provide a statistical summary of the wildfires DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the weather dataset\n",
    "file_weather = f\"{main_path}/HistoricalWeather.csv\"\n",
    "weather_df = pd.read_csv(file_weather)  # Load the weather dataset into a pandas DataFrame\n",
    "\n",
    "# Rename columns\n",
    "weather_df = weather_df.rename(\n",
    "    columns={\n",
    "        \"count()[unit: km^2]\": \"Area\",\n",
    "        \"min()\": \"Min\",\n",
    "        \"max()\": \"Max\",\n",
    "        \"mean()\": \"Mean\",\n",
    "        \"variance()\": \"Variance\",\n",
    "    }\n",
    ")\n",
    "\n",
    "weather_df[\"Date\"] = pd.to_datetime(weather_df[\"Date\"])  # Convert the 'Date' column in the weather DataFrame to datetime format\n",
    "\n",
    "weather_df.head()  # Display the first few rows of the weather DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the weather DataFrame to reformat the data for analysis\n",
    "df_pivot = weather_df.pivot_table(\n",
    "    values=[\"Min\", \"Max\", \"Mean\", \"Variance\"],\n",
    "    index=[\"Date\", \"Region\"],\n",
    "    columns=[\"Parameter\"],\n",
    ")\n",
    "\n",
    "# Reset the index of the pivoted DataFrame to flatten the data structure\n",
    "df_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Rename columns in the pivoted DataFrame for easier access\n",
    "df_pivot.columns = [\n",
    "    col[0] if not (col[1]) else \"{1}_{0}\".format(*col)\n",
    "    for col in df_pivot.columns.values\n",
    "]\n",
    "\n",
    "# Rearrange the data and columns in the pivoted DataFrame\n",
    "params = df_pivot.columns.tolist()[3:]\n",
    "params.sort()\n",
    "weather_data = df_pivot[df_pivot.columns.tolist()[:3] + params].copy()\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the vegetation index dataset\n",
    "file_wildfires = f\"{main_path}/VegetationIndex.csv\"\n",
    "ndvi_df = pd.read_csv(file_wildfires)  # Load the vegetation index dataset into a pandas DataFrame\n",
    "\n",
    "# Convert the 'Date' column in the vegetation index DataFrame to datetime format\n",
    "ndvi_df[\"Date\"] = pd.to_datetime(ndvi_df[\"Date\"])\n",
    "\n",
    "print(ndvi_df.dtypes)  # Display the data types of the columns in the vegetation index DataFrame\n",
    "ndvi_df.head()  # Display the first few rows of the vegetation index DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the weather forecasts dataset\n",
    "file_forecasts = f\"{main_path}/HistoricalWeatherForecasts.csv\"\n",
    "forecasts_df = pd.read_csv(file_forecasts)  # Load the weather forecasts dataset into a pandas DataFrame\n",
    "\n",
    "# Convert the 'Date' column in the weather forecasts DataFrame to datetime format\n",
    "forecasts_df[\"Date\"] = pd.to_datetime(forecasts_df[\"Date\"])\n",
    "\n",
    "forecasts_df.head()  # Display the first few rows of the weather forecasts DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the historical fire and weather data into a single DataFrame\n",
    "df_all = wildfires_df.merge(weather_data, how=\"left\", on=[\"Date\", \"Region\"])\n",
    "df_all.describe().transpose()  # Provide a statistical summary of the merged DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns  # Display column names of the merged DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.isna().sum()  # Calculate and display the number of missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display correlation of all columns with 'Estimated_fire_area'\n",
    "df_all.corr()[\"Estimated_fire_area\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged DataFrame to a CSV file\n",
    "df_all.to_csv(\"dataset.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.shape  # Display the shape (dimensions) of the merged DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_df.shape  # Display the shape (dimensions) of the vegetation index DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the merged DataFrame with the vegetation index DataFrame\n",
    "df_temp = df_all.merge(ndvi_df, how=\"inner\", on=[\"Date\", \"Region\"])\n",
    "\n",
    "df_temp.describe().transpose()  # Provide a statistical summary of the newly merged DataFrame\n",
    "df_temp.to_csv(\"dataset1.csv\", index=False, encoding=\"utf-8\")  # Save the newly merged DataFrame to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for correlation analysis by dropping certain columns\n",
    "df_corr = df_all.drop(\n",
    "    [\n",
    "        \"Region\",\n",
    "        \"Date\",\n",
    "        \"Mean_confidence\",\n",
    "        \"Std_confidence\",\n",
    "        \"Var_confidence\",\n",
    "        \"Count\",\n",
    "        \"Replaced\",\n",
    "    ],\n",
    "    axis=1,\n",
    ").copy()\n",
    "\n",
    "# Create a plot for the correlation matrix\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.heatmap(df_corr.corr(), cmap=\"coolwarm\", annot=True, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for a specific region ('NSW') and create a copy\n",
    "df_all2 = df_all[df_all[\"Region\"] == \"NSW\"].copy()\n",
    "df_all2.drop_duplicates(inplace=True)  # Drop duplicate rows\n",
    "df_all2.reset_index(drop=True, inplace=True)  # Reset the index of the filtered DataFrame\n",
    "df_all2 = df_all2.dropna(how=\"any\")  # Drop rows with any missing values\n",
    "\n",
    "# Drop certain columns from the filtered DataFrame and create a copy\n",
    "df_all2 = df_all2.drop(\n",
    "    [\n",
    "        \"Date\",\n",
    "        \"Region\",\n",
    "        \"Mean_confidence\",\n",
    "        \"Std_confidence\",\n",
    "        \"Var_confidence\",\n",
    "        \"Count\",\n",
    "        \"Replaced\",\n",
    "    ],\n",
    "    axis=1,\n",
    ").copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
